{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The dataset here is the updated metadata data, containing 415 movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal \n",
    "Creating a content based and hybrid recommender system based on the \"overview\" feature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "#from gensim.test.utils import common_texts, get_tmpfile\n",
    "\n",
    "\n",
    "from six.moves import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1 Importing data and preparing data for learning a word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m=pd.read_csv('movies_metadata_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2274 entries, 0 to 2273\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0             2274 non-null   int64  \n",
      " 1   adult                  2274 non-null   bool   \n",
      " 2   belongs_to_collection  771 non-null    object \n",
      " 3   budget                 2274 non-null   int64  \n",
      " 4   genres                 2274 non-null   object \n",
      " 5   homepage               1187 non-null   object \n",
      " 6   id                     2274 non-null   float64\n",
      " 7   imdb_id                2274 non-null   object \n",
      " 8   original_language      2274 non-null   object \n",
      " 9   original_title         2274 non-null   object \n",
      " 10  overview               2274 non-null   object \n",
      " 11  popularity             2274 non-null   float64\n",
      " 12  poster_path            2274 non-null   object \n",
      " 13  production_companies   2274 non-null   object \n",
      " 14  production_countries   2274 non-null   object \n",
      " 15  release_date           2274 non-null   object \n",
      " 16  revenue                2274 non-null   float64\n",
      " 17  runtime                2274 non-null   float64\n",
      " 18  spoken_languages       2274 non-null   object \n",
      " 19  status                 2274 non-null   object \n",
      " 20  tagline                2170 non-null   object \n",
      " 21  title                  2274 non-null   object \n",
      " 22  video                  2274 non-null   bool   \n",
      " 23  vote_average           2274 non-null   float64\n",
      " 24  vote_count             2274 non-null   float64\n",
      "dtypes: bool(2), float64(6), int64(2), object(15)\n",
      "memory usage: 413.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_m.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Restricting the dataframe to only id, title and overview columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_r=df_m[[\"id\",\"title\" ,\"overview\"]].dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_r=df_m_r.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2274, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Removing ASCII characters, converting lower case, removing stop words, html and punctuation from overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_r['overview']=df_m_r['overview'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utitlity functions for removing ASCII characters, converting lower case, removing stop words, html and punctuation from description\n",
    "\n",
    "def _removeNonAscii(s):\n",
    "    return \"\".join(i for i in s if  ord(i)<128)\n",
    "\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "df_m_r['overview']=df_m_r['overview'].fillna(\"\")\n",
    "df_m_r['overview_n'] = df_m_r['overview'].apply(_removeNonAscii)\n",
    "df_m_r['overview_n'] = df_m_r.overview_n.apply(make_lower_case)\n",
    "df_m_r['overview_n'] = df_m_r.overview_n.apply(remove_stop_words)\n",
    "df_m_r['overview_n'] = df_m_r.overview_n.apply(remove_punctuation)\n",
    "df_m_r['overview_n'] = df_m_r.overview_n.apply(remove_html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>overview_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>862.0</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>led woody andy s toys live happily room andy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8844.0</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>siblings judy peter discover enchanted board g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>949.0</td>\n",
       "      <td>Heat</td>\n",
       "      <td>Obsessive master thief, Neil McCauley leads a ...</td>\n",
       "      <td>obsessive master thief neil mccauley leads top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>710.0</td>\n",
       "      <td>GoldenEye</td>\n",
       "      <td>James Bond must unmask the mysterious head of ...</td>\n",
       "      <td>james bond must unmask mysterious head janus s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>524.0</td>\n",
       "      <td>Casino</td>\n",
       "      <td>The life of the gambling paradise – Las Vegas ...</td>\n",
       "      <td>life gambling paradise las vegas dark mafia un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id      title                                           overview  \\\n",
       "0   862.0  Toy Story  Led by Woody, Andy's toys live happily in his ...   \n",
       "1  8844.0    Jumanji  When siblings Judy and Peter discover an encha...   \n",
       "2   949.0       Heat  Obsessive master thief, Neil McCauley leads a ...   \n",
       "3   710.0  GoldenEye  James Bond must unmask the mysterious head of ...   \n",
       "4   524.0     Casino  The life of the gambling paradise – Las Vegas ...   \n",
       "\n",
       "                                          overview_n  \n",
       "0  led woody andy s toys live happily room andy s...  \n",
       "1  siblings judy peter discover enchanted board g...  \n",
       "2  obsessive master thief neil mccauley leads top...  \n",
       "3  james bond must unmask mysterious head janus s...  \n",
       "4  life gambling paradise las vegas dark mafia un...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2274, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df_m_r.reset_index().drop('overview', axis=1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Saving prepared data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save prepared data to match with ratings data to use in collaborative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('content_based_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Learn the word embedding using word2vec and improved word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Finding idf weights of updated overview column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the tfidfVectorizer model\n",
    "tfidf = TfidfVectorizer(analyzer='word',stop_words='english',ngram_range=(1, 2),min_df =0)\n",
    "tfidf_matrix = tfidf.fit_transform(df1['overview_n'])\n",
    "tfidf_feature = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68739\n",
      "(2274, 68739)\n"
     ]
    }
   ],
   "source": [
    "#info about tf_idf matrix shape \n",
    "print(len(tfidf_feature))\n",
    "print(tfidf_matrix.shape)\n",
    "#print(tfidf_feature[:10])\n",
    "#tfidf_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.34344097 7.63112304 8.03658815 ... 8.03658815 8.03658815 8.03658815]\n",
      "(68739,)\n"
     ]
    }
   ],
   "source": [
    "#here we list the idf for each element of vocabulary\n",
    "print(tfidf.idf_)\n",
    "print(tfidf.idf_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_list = dict(zip(tfidf.get_feature_names(), list(tfidf.idf_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_words = tfidf_matrix.sum(axis=0)\n",
    "#print(total_words.shape)\n",
    "#print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Learning word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating a list of rows of overview_n column\n",
    "corpus = []\n",
    "for words in df1['overview_n']:\n",
    "    corpus.append(words.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating word2vec model \n",
    "model = Word2Vec(corpus,size = 100, window=5, min_count = 2, workers = -1)\n",
    "model.save(\"word2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking vocabulary\n",
    "vocabulary = model.wv.vocab\n",
    "#model.wv['put'] return a 100 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get all the vocabularies in the model:\n",
    "vocabulary.keys()\n",
    "#to find the embedding vector for a word \n",
    "model.wv['dinosaurs'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Finding the word2vec embedding vector for each text in the corpus (word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors():\n",
    "    \n",
    "    # Creating a list for storing the vectors (description into vectors)\n",
    "    global word_embeddings\n",
    "    word_embeddings = []\n",
    "    for line in df1['overview_n']:\n",
    "        avgword2vec = None\n",
    "        count = 0\n",
    "        for word in line.split():      \n",
    "            if word in model.wv.vocab:\n",
    "                count += 1\n",
    "                if avgword2vec is None:\n",
    "                    avgword2vec = model[word]\n",
    "                else:\n",
    "                    avgword2vec = avgword2vec + model[word]                \n",
    "        if avgword2vec is not None:\n",
    "            avgword2vec = avgword2vec / count\n",
    "            word_embeddings.append(avgword2vec)\n",
    "        else:\n",
    "            word_embeddings.append([0]*100)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-f70caa9e6965>:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  avgword2vec = model[word]\n",
      "<ipython-input-21-f70caa9e6965>:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  avgword2vec = avgword2vec + model[word]\n"
     ]
    }
   ],
   "source": [
    "vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Finding the tf-idf weighted word2vec embedding vector for each text in the corpus (tfdif_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building TF-IDF Word2Vec Model\n",
    "def tfdifvectors():\n",
    "    # Storing the TFIDF Word2Vec embeddings\n",
    "    global tfidf_vectors\n",
    "    tfidf_vectors = []; \n",
    "    line = 0;\n",
    "    # for each book description\n",
    "    for desc in corpus: \n",
    "        # Word vectors are of zero length (Used 100 dimensions)\n",
    "        sent_vec = np.zeros(100) \n",
    "        # num of words with a valid vector in the movie overview\n",
    "        weight_sum =0 \n",
    "        # for each word in the movieoverview\n",
    "        for word in desc: \n",
    "            if word in model.wv.vocab and word in tfidf_feature:\n",
    "                vec = model.wv[word]\n",
    "                tf_idf = tfidf_list[word] * (desc.count(word) / len(desc))\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                weight_sum += tf_idf\n",
    "        if weight_sum != 0:\n",
    "            sent_vec /= weight_sum\n",
    "            tfidf_vectors.append(sent_vec)\n",
    "        else: \n",
    "            tfidf_vectors.append(sent_vec)\n",
    "        line += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdifvectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Defining a content based recommender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendations(title,cosine_similarities):\n",
    "    indices = pd.Series(df1.index, index = df1['title']).drop_duplicates()\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
    "    sim_scores = sim_scores[1:6]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    recommend = df1['title'].iloc[movie_indices]\n",
    "    return recommend\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Recommending 5 movies similar to \"GoldenEye\" movie using word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we find the cosine similariy matrix of word_embeddings,a matrix of shape (13715, 13715)\n",
    "\n",
    "cosine_similarities_we =cosine_similarity(word_embeddings, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy Story\n",
      "460                                  Toy Story 2\n",
      "623     Harry Potter and the Philosopher's Stone\n",
      "1936                                       Selma\n",
      "755                                          Elf\n",
      "1358                                 Toy Story 3\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "title=df1['title'][0]\n",
    "print(title)\n",
    "print(recommendations(title,cosine_similarities_we))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Recommending 5 movies similar to \"GoldenEye\" movie using tfdifvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities_tf=cosine_similarity( tfidf_vectors,  tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460                Toy Story 2\n",
      "1358               Toy Story 3\n",
      "913     The 40 Year Old Virgin\n",
      "716        The Matrix Reloaded\n",
      "1936                     Selma\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(recommendations(title,cosine_similarities_tf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Defining a hybrid recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a hybrid recommender using learned svd model and knn model in collborative file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommender_hybrid(title,cosine_similarities,model,user_Id,df_meta):\n",
    "    indices = pd.Series(df_meta.index, index = df_meta['title']).drop_duplicates()\n",
    "    idx = indices[title]\n",
    "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
    "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
    "    sim_scores=sim_scores[1:26]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    data=df_meta[df_meta['id'].isin(movie_indices)]\n",
    "    data['prediction']=data['id'].apply(lambda x: model.predict(uid=user_Id,iid=x).est)\n",
    "    data= data.sort_values('prediction', ascending=False)\n",
    "    recommend = data.iloc[:5]['title']\n",
    "    return recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model = pickle.load(open('recommender-svd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-9a23998819b2>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['prediction']=data['id'].apply(lambda x: model.predict(uid=user_Id,iid=x).est)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1002                   Crank\n",
       "15       From Dusk Till Dawn\n",
       "259     Seven Years in Tibet\n",
       "143      A Fish Called Wanda\n",
       "614         Mulholland Drive\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender_hybrid(title,cosine_similarities_we,svd_model,user_Id=24,df_meta=df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = pickle.load(open('recommender-knn', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-9a23998819b2>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['prediction']=data['id'].apply(lambda x: model.predict(uid=user_Id,iid=x).est)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "259     Seven Years in Tibet\n",
       "143      A Fish Called Wanda\n",
       "1002                   Crank\n",
       "15       From Dusk Till Dawn\n",
       "614         Mulholland Drive\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommender_hybrid(title,cosine_similarities_we,knn_model,user_Id=24,df_meta=df1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
